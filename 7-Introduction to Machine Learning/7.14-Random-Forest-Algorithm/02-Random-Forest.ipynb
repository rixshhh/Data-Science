{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f3a4b0",
   "metadata": {},
   "source": [
    "# Random Forest (Classification & Regression)\n",
    "\n",
    "**Random Forest** is an **Ensemble Learning algorithm** that uses **multiple Decision Trees to make predictions**.\n",
    "\n",
    "It works using the concept of **Bagging (Bootstrap Aggregating) + Random Feature Selection**.\n",
    "\n",
    "---\n",
    "\n",
    "## Definition\n",
    "\n",
    "### *Random Forest Classification* \n",
    "\n",
    "A Random Forest classifier builds **many decision trees**, and each tree votes for a class.\n",
    "The final class is assigned based on **majority voting**.\n",
    "\n",
    "Example:\n",
    "\n",
    "If 100 trees are trained, and 60 vote for Class A → final prediction = Class A.\n",
    "\n",
    "\n",
    "### *Random Forest Regression*\n",
    "\n",
    "A Random Forest regressor builds many regression trees, and each tree predicts a number.\n",
    "The final output is the average of all tree predictions.\n",
    "\n",
    "Example:\n",
    "\n",
    "If 5 trees predict: [10, 12, 11, 13, 14]\n",
    "\n",
    "→ Final prediction = (10 + 12 + 11 + 13 + 14) / 5 = 12\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18430f8",
   "metadata": {},
   "source": [
    "## Why Do Random Forests Work Well?\n",
    "\n",
    "Because each tree:\n",
    "\n",
    "- Trains on a **different subset of data** (bootstrap sampling)\n",
    "\n",
    "- Gets a **random subset of features at each split**\n",
    "\n",
    "That makes each tree slightly different → reducing correlation, and resulting in:\n",
    "\n",
    "✔️ Less overfitting\n",
    "\n",
    "✔️ Higher accuracy\n",
    "\n",
    "✔️ More stable predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d1292",
   "metadata": {},
   "source": [
    "## Core Concepts of Random Forest\n",
    "\n",
    "### 1️⃣ Bootstrap Sampling\n",
    "\n",
    "Each tree is trained on a random sample of the dataset with replacement.\n",
    "\n",
    "### 2️⃣ Random Feature Selection\n",
    "\n",
    "At each split, the tree randomly selects only a few features to choose the best split.\n",
    "\n",
    "### 3️⃣ Ensemble Prediction\n",
    "\n",
    "- Classification → Majority Vote\n",
    "\n",
    "- Regression → Average Prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08880e4",
   "metadata": {},
   "source": [
    "### Advantage\n",
    "\n",
    "| Advantage                            | Explanation                      |\n",
    "| ------------------------------------ | -------------------------------- |\n",
    "| High Accuracy                        | Due to combination of many trees |\n",
    "| Reduces Overfitting                  | Each tree sees different data    |\n",
    "| Handles Numerical + Categorical Data | Works on mixed data              |\n",
    "| Works Well on Large Datasets         | Fast and parallelizable          |\n",
    "| No Need for Feature Scaling          | Trees don’t need normalization   |\n",
    "\n",
    "### Disadvantage\n",
    "\n",
    "| Disadvantage              | Explanation                             |\n",
    "| ------------------------- | --------------------------------------- |\n",
    "| Less Interpretable        | Harder to understand than a single tree |\n",
    "| Slower than a single Tree | Many trees = more computation           |\n",
    "| More Memory Use           | Stores many models                      |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Random Forest HyperParameter\n",
    "\n",
    "| Parameter           | Meaning                                      |\n",
    "| ------------------- | -------------------------------------------- |\n",
    "| `n_estimators`      | Number of trees in the forest                |\n",
    "| `max_depth`         | Maximum depth of each tree                   |\n",
    "| `criterion`         | impurity measure (gini, entropy, mse)        |\n",
    "| `max_features`      | Number of features to consider at each split |\n",
    "| `min_samples_split` | Minimum samples to split a node              |\n",
    "| `min_samples_leaf`  | Minimum samples in a leaf node               |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ba1a8",
   "metadata": {},
   "source": [
    "## Row Sampling (Bootstrap Sampling)\n",
    "\n",
    "Row sampling means selecting rows (samples) from the dataset randomly with replacement.\n",
    "\n",
    "✔️ Used In:\n",
    "\n",
    "- Bagging\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "- Bootstrap-based models\n",
    "\n",
    "✔️ Why do we do this?\n",
    "\n",
    "- To create different datasets for each tree\n",
    "\n",
    "- Makes trees less correlated\n",
    "\n",
    "- Reduces overfitting\n",
    "\n",
    "- Improves model stability\n",
    "\n",
    "✔️ Example:\n",
    "\n",
    "Original dataset = 100 rows\n",
    "\n",
    "Each tree trains on ≈ 63–70 rows (some repeated due to replacement).\n",
    "\n",
    "--- \n",
    "\n",
    "## 2️⃣ Column Sampling (Feature Sampling)\n",
    "\n",
    "Column sampling means selecting a subset of features randomly when making a split in a decision tree.\n",
    "\n",
    "✔️ Used In:\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "- Extra Trees\n",
    "\n",
    "- Gradient Boosting (optional)\n",
    "\n",
    "✔️ Why do we do this?\n",
    "\n",
    "- Forces each tree to learn different feature combinations\n",
    "\n",
    "- Reduces correlation between trees\n",
    "\n",
    "- Improves generalization\n",
    "\n",
    "- Helps when many features are irrelevant\n",
    "\n",
    "✔️ Example:\n",
    "\n",
    "Total features = 20\n",
    "\n",
    "Random Forest may choose only sqrt(20) ≈ 4 features at each split.\n",
    "\n",
    "--- \n",
    "\n",
    "## 3️⃣ Combined Sampling (Row + Column Sampling)\n",
    "\n",
    "This means applying both:\n",
    "\n",
    "        ✔️ Row sampling\n",
    "\n",
    "        ✔️ Column sampling\n",
    "\n",
    "at the same time.\n",
    "\n",
    "This is what Random Forest does.\n",
    "\n",
    "✔️ Benefits:\n",
    "\n",
    "- Maximum diversity between trees\n",
    "\n",
    "- Better performance\n",
    "\n",
    "- Stronger ensemble\n",
    "\n",
    "- Less overfitting\n",
    "\n",
    "✔️ Example:\n",
    "\n",
    "- Rows → Random bootstrap samples\n",
    "\n",
    "- Columns → Only a subset chosen at each split\n",
    "\n",
    "--- \n",
    "\n",
    "## 4️⃣ Feature Sampling (Same as Column Sampling)\n",
    "\n",
    "- Feature sampling = Column sampling.\n",
    "\n",
    "- It means selecting some features randomly at:\n",
    "\n",
    "- Node level (most common)\n",
    "\n",
    "- Tree level (using max_features)\n",
    "\n",
    "- Split level\n",
    "\n",
    "Types:\n",
    "\n",
    "- sqrt(n_features) → default for classification\n",
    "\n",
    "- log2(n_features) → stronger randomization\n",
    "\n",
    "- all features → no sampling (pure decision tree)\n",
    "\n",
    "--- \n",
    "\n",
    "## All Concepts in One Table\n",
    "\n",
    "| Concept               | What is sampled?        | Used in                | Purpose                                |\n",
    "| --------------------- | ----------------------- | ---------------------- | -------------------------------------- |\n",
    "| **Row Sampling**      | Samples/rows            | Bagging, Random Forest | Reduce overfitting, increase diversity |\n",
    "| **Column Sampling**   | Features                | Random Forest          | Reduce correlation, improve stability  |\n",
    "| **Combined Sampling** | Rows + Features         | Random Forest          | Best performance                       |\n",
    "| **Feature Sampling**  | Same as column sampling | Decision Trees, RF     | Avoid dominated features               |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
