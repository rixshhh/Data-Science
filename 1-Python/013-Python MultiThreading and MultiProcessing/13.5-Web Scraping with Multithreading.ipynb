{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90060682",
   "metadata": {},
   "source": [
    "# üï∏Ô∏è Web Scraping with Multithreading in Python\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates a real-world use case of **multithreading** for I/O-bound tasks ‚Äî specifically, **web scraping**.\n",
    "\n",
    "When scraping multiple web pages, network requests can be slow. Instead of waiting for each page to download sequentially, we can use **threads** to fetch them concurrently, improving efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5edfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests\n",
      "\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   -------- ------------------------------- 1/5 [idna]\n",
      "   -------- ------------------------------- 1/5 [idna]\n",
      "   -------- ------------------------------- 1/5 [idna]\n",
      "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
      "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
      "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
      "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
      "   -------------------------------- ------- 4/5 [requests]\n",
      "   -------------------------------- ------- 4/5 [requests]\n",
      "   -------------------------------- ------- 4/5 [requests]\n",
      "   -------------------------------- ------- 4/5 [requests]\n",
      "   ---------------------------------------- 5/5 [requests]\n",
      "\n",
      "Successfully installed certifi-2025.10.5 charset_normalizer-3.4.4 idna-3.11 requests-2.32.5 urllib3-2.5.0\n",
      "Requirement already satisfied: beautifulsoup4 in f:\\clg\\data science\\1-python\\venv\\lib\\site-packages (4.14.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in f:\\clg\\data science\\1-python\\venv\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in f:\\clg\\data science\\1-python\\venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b445aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download: https://python.langchain.com/v0.2/docs/introduction/\n",
      "Starting download: https://python.langchain.com/v0.2/docs/concepts/\n",
      "Starting download: https://python.langchain.com/v0.2/docs/tutorials/\n",
      "‚úÖ Fetched 3814 characters from https://python.langchain.com/v0.2/docs/tutorials/\n",
      "‚úÖ Fetched 3814 characters from https://python.langchain.com/v0.2/docs/concepts/\n",
      "‚úÖ Fetched 3814 characters from https://python.langchain.com/v0.2/docs/introduction/\n",
      "\n",
      "All web pages fetched in 1.99 seconds\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://python.langchain.com/v0.2/docs/introduction/\",\n",
    "    \"https://python.langchain.com/v0.2/docs/concepts/\",\n",
    "    \"https://python.langchain.com/v0.2/docs/tutorials/\",\n",
    "]\n",
    "\n",
    "def fetch_content(url):\n",
    "    \"\"\"Fetch and parse content from a single URL.\"\"\"\n",
    "    try:\n",
    "        print(f\"Starting download: {url}\")\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # raise error for bad responses\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        print(f\"‚úÖ Fetched {len(soup.text)} characters from {url}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error fetching {url}: {e}\")\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create and start threads\n",
    "    threads = []\n",
    "    for url in urls:\n",
    "        thread = threading.Thread(target=fetch_content, args=(url,))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    print(f\"\\nAll web pages fetched in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa839043",
   "metadata": {},
   "source": [
    "### ‚úÖ Output\n",
    "\n",
    "After running the cell above, you‚Äôll see concurrent downloads and timing showing how multithreading speeds up I/O-bound operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64190ac",
   "metadata": {},
   "source": [
    "### ‚ö° ThreadPoolExecutor Alternative\n",
    "\n",
    "Instead of manually managing threads, Python‚Äôs `concurrent.futures.ThreadPoolExecutor` provides a simpler API for multithreading:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577eeabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 3814 characters from https://python.langchain.com/v0.2/docs/concepts/\n",
      "Fetched 3814 characters from https://python.langchain.com/v0.2/docs/introduction/\n",
      "Fetched 3814 characters from https://python.langchain.com/v0.2/docs/tutorials/\n",
      "\n",
      "All pages fetched in 2.01 seconds\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "urls = [\n",
    "    \"https://python.langchain.com/v0.2/docs/introduction/\",\n",
    "    \"https://python.langchain.com/v0.2/docs/concepts/\",\n",
    "    \"https://python.langchain.com/v0.2/docs/tutorials/\",\n",
    "]\n",
    "\n",
    "def fetch_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        print(f\"Fetched {len(soup.text)} characters from {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "start = time.time()\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    executor.map(fetch_content, urls)\n",
    "\n",
    "print(f\"\\nAll pages fetched in {time.time() - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30df81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
